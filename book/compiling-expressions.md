^title Compiling Expressions
^part A Bytecode Virtual Machine

> There are no happy endings.
>
> Endings are the saddest part,
>
> So just give me a happy middle
>
> And a very happy start.
> <cite>Shel Silverstein</cite>

**todo: more illustrations**

**todo: mention use "compile" and "parse" interchangeably**

This chapter is exciting for not one, not two, but three reasons. First, it
fills in the final missing section of the execution pipeline of our VM. Once in
place, we can pipe a user's source code from scanning all the way through to
executing it.

**todo: illustrate last section in pipeline**

Second, we get to write an actual, honest-to-God *compiler*. It parses source
code and outputs a low-level series of binary instructions. Sure, it's <span
name="wirth">bytecode</span> and not some chip's native instruction set, but
it's still fairly close to the metal. We're about to be real language hackers.

<aside name="wirth">

Bytecode was good enough for Niklaus Wirth, and no one questions his compiler
street cred.

</aside>

<span name="pratt">Third</span> and finally, I get to show you one of my
absolute favorite techniques -- Vaughan Pratt's "top-down operator precedence
parsing". It's the most elegant way I know to parse expressions. It gracefully
handles prefix operators, postfix, infix, *mixfix*, any kind of *-fix* you got.
It deals with precedence and associativity without breaking a sweat. I love it.

<aside name="pratt">

Top-down operator precedence parsers exist as a strange sort of tribal lore. No
compiler or language book I've read teaches them. Academia is very focused on
generated parsers, and Pratt's technique is for hand-written ones, so it's
mostly overlooked.

But in industry, where hand-rolled parsers are actually quite common, you'd be
surprised how many people know it. Ask where they learned it, and it's always,
"Oh, I worked on this compiler years ago that used one and they got the idea
from this other old front end..."

</aside>

As usual, before we get to the fun stuff, we've got some preliminaries to work
through. You have to eat your vegetables before you get dessert. First, let's
ditch that temporary scaffolding we wrote for testing the scanner and replace it
with something more useful:

^code interpret-chunk (1 before, 1 after)

We create a new empty chunk and pass it over to the compiler. The compiler will
take the users program and fill in the chunk with the generated bytecode for it.
At least, that's what it will do if the program doesn't have any compile errors.
If it does encounter an error, `compile()` returns `false` and we discard the
unusable chunk.

Otherwise, we send the completed chunk over to the VM and execute it. When that
completes, we free the chunk and we're done.

As you can see, the signature to `compile()` is different now:

^code compile-h (2 before, 2 after)

We pass in the chunk where the compiler will output its code, and the function
returns whether or not compilation succeeded. Over in the implementation too:

^code compile-signature (1 before, 1 after)

That call to `initScanner()` is the only line that survives this chapter. Tear
out that temporary code we wrote to test the scanner and replace it with these
three lines:

^code compile-chunk (1 before, 1 after)

The call to `advance()` "primes the pump" on the scanner. It scans the first
token in the program and stores it as the current token in the compiler. Then we
parse a single expression. We don't have statements yet. We'll revisit this when
we [add them in a few chapters][globals]. Once we compile the entire expression,
we should have reached the end of the source code, so we make sure we find the
sentinel EOF token at the end.

[globals]: global-variables.html

We're going to spend the rest of the chapter making this function work,
especially that little `expression()` call. Normally, we'd go right to that
function definition and see what it calls. Then do *those* functions and so one,
working our way through the implementation from top to bottom.

This chapter is a little <span name="blog">different</span>. Pratt's parsing
technique is remarkably simple once you have it all loaded in your head, but
it's a little tricky to break into bite-sized pieces. It's recursive, of course,
which is part of the problem. But it also relies on a big table of data. As we
build up the algorithm, that table gets additional columns. I don't want to
force you rewrite 40-something lines of code each time we extend the table.

<aside name="blog">

If this chapter isn't clicking with you and you'd like another approach to the
concepts, I wrote an article that teaches the same algorithm but using Java and
an object-oriented style: [Pratt Parsing: Expression Parsing Made Easy][blog].

[blog]: http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/

</aside>

So we're going to work our way into the core of the parser from outside and
cover all of the surrounding bits before we get to the juicy center. This will
require a little more patience and mental scratch space than most chapters, but
it's the best I could do. I think you'll find it's worth the effort.

## Single-Pass Compilation

A compiler has roughly two jobs. It parses the user's source code to understand
what it means and report any errors. Then it takes that semantic model and
outputs low-level instructions that produce the same semantics.

Many languages split those two roles into two separate <span
name="passes">passes</span> in the implementation. The parser produces an AST --
just like jlox does -- and then the "code generator" traverses the AST and
outputs target code.

<aside name="passes">

In fact, most sophisticated optimizing compilers have a heck of a lot more than
two passes. Determining not just *what* optimization passes to have, but how to
order them to squeeze the most performance out of the compiler -- since the
optimizations often interact in complex ways -- is somewhere between an "open
area of research" and a "black art".

</aside>

In clox, we're going to take an old school approach and merge these two passes
into one. Back in the day, language hackers did this because computers literally
didn't have enough memory to store an entire source file's AST. We're doing it
because it keeps our compiler simpler, which is a real asset when programming in
C.

A "single-pass compiler" like we're going to build doesn't work well for all
languages. The compiler only has a peephole-sized view into the user's program
while it's parsing source and generating code. So the language needs to be
designed such that you can figure out what output code to generation without
needing a lot of surrounding context and without needing to look ahead at later
parts of the program.

<span name="lox">Fortunately</span>, tiny, dynamically-typed Lox is well-suited
to single-pass compilation. It's also a very fast way to compile, which is nice
for an interpreted language. Unfortunately, doing everything in one pass doesn't
leave a lot of room for optimizations, so the resulting bytecode isn't super
fast. As always, there are trade-offs.

<aside name="lox">

Not that this should come as much of a surprise. I did design the language for
this book after all.

</aside>

What this means in practical terms is that our "compiler" C module has
functionality you'll recognize from jlox for parsing -- consuming tokens,
matching expected token types, etc. And it also has functions for code gen --
emitting bytecode and adding constants to the destination chunk.

We'll build those two ends up first. Then we'll stitch them together with the
code in the middle that parses Lox's particular grammar and outputs bytecode for
its semantics. This is a little backwards from how most chapters work. But it
means that by the time we get to the tricky core of the top-down operator
precedence parser, we will have already assembled all of the other tools we
need.

## Parsing Tokens

First up, the front half of the compiler. This function should give you
déjà vu from our jlox days:

^code advance

Just like in jlox, it advances to the next token in the source code. It tells
the scanner to scan the next token and stores it for later use. Before doing
that, it takes the old current token and stashes that in another field. That
will come in handy later so that we can inspect the lexeme after we've matched a
token.

The code to read the next token is wrapped in a loop. Remember, our scanner
doesn't report lexical errors. Instead, it creates the special "error tokens"
and leaves it up to the parser to detect those and report the error.

We do that here. Then we read the next token, or as many next tokens as we need
to until we run out or hit a non-error one. That way, the rest of the parser
doesn't have to deal with error tokens floating around confusing things.

The current and previous token are stored in this new struct:

^code parser (1 before, 2 after)

Like we did in other modules, we have a single global variable of this struct
type. That way, we don't need to pass the state around from function to function
in the compiler.

### Handling syntax errors

If the scanner handed us an error token, we need to actually tell the user:

^code error-at-current

This function reports the given error message. We pull the location out of the
current token in order to tell the user where the error occurred. More often,
we'll report an error at the location of the token we just consumed, so we also
define:

^code error

Both of these are built on this fundamental function:

^code error-at

This one takes the token as a parameter because sometimes we'll want to report
errors at the *previous* token's position. First, we use that token to print out
where the error occurred. We try to show the lexeme if it's human-readable.

<aside name="format">

That `%.*s` in the format string is a neat feature you might not know. Usually,
you the output precision -- the maximum number of characters to show -- as a
number inside the format string. Calling `printf()` with `%.3s` as the format
prints the first three characters of the given string.

Using `*` instead lets you pass the precision in as an argument. So that
`fprintf()` call prints the first `token->length` characters of the string at
`token->start`. We need to limit the length like that because the lexeme points
into the original source string and doesn't have a terminator at the end.

</aside>

Then we print the error message itself. After that, we set this `hadError` flag.
That records whether any errors occurred during compilation. It's stored in the
parser struct:

^code had-error-field (1 before, 1 after)

Earlier I said that `compile()` should return `false` if an error occurred. Now
we can make it do that:

^code return-had-error (1 before, 1 after)

I've got another flag for error handling to introduce. We want to avoid error
cascades. After a syntax error is detected, if the parser is confused about
where it is in the user's program, we don't want it to spew out a whole pile of
meaningless knock-on errors.

We fixed that in jlox using panic mode error recovery. In the Java interpreter,
we threw an exception to unwind out of all of the parser code to a point where
we could skip tokens and resynchronize. We don't have <span
name="setjmp">exceptions</span> in C.

<aside name="setjmp">

There is `setjmp()` and `longjmp()`, but I'd rather not go there. Those make it
too easy to leak resources, forget to clean up memory, or cause other nasty
headaches.

</aside>

Instead, we'll do a little smoke and mirrors. We add a flag to track whether
we're currently in panic mode:

^code panic-mode-field (1 before, 1 after)

When an error first occurs, we set it:

^code set-panic-mode (1 before, 1 after)

After that, we go ahead and keep compiling as normal as if the error never
occurred. The bytecode will never get executed, so it's harmless to keep on
trucking. The trick is that while the panic mode flag is set, we simply suppress
any other errors that get detected:

^code check-panic-mode (1 before, 1 after)

There's a good chance the parser will get confused and go off in the weeds, but
the user won't know because the errors all get swallowed. Panic mode ends when
the parser reaches a synchronization point. For Lox, we chose statement
boundaries, so when we later add those to our compiler, we'll clear the flag.

These new fields need to be initialized:

^code init-parser-error (1 before, 1 after)

And to display the errors, we need a standard header:

^code compiler-include-stdlib (1 before, 2 after)

There's one last parsing function, another familiar one from jlox:

^code consume

It's similar to `advance()` in that reads the next token. But first, it
validates that the token has an expected type. If not, it reports an error. This
function is the foundation of most syntax errors in the compiler.

OK, that's enough basic parser stuff to get up and moving. Let's hop to the
other end and look at the bytecode.

## Emitting Bytecode

After we've done some parsing and understood a piece of the user's program, the
next step is to emit the series of bytecode instructions that represents that
program's semantics. The core function writes a single byte to the chunk:

^code emit-byte

It's hard to believe great things will flow through such a simple operation. It
writes the given byte, which may be an opcode or an operand to an instruction.
It sends in the current token's line information so that runtime errors
are associated with that line. This assumes we write the byte as soon as we can
after reading a given token.

The chunk that we're writing gets passed into `compile()`, but it needs to make
its way to this function. To do that, we use this intermediary function:

^code compiling-chunk (1 before, 1 after)

Right now, the chunk pointer is stored in a module level variable like we do
other global state. Later, when we start compiling user-defined functions, the
notion of "current chunk" gets more complicated. To avoid having to go back and
change a lot of code, we encapsulate that logic in this function.

We need to store the chunk in the module variable before we write any bytecode:

^code init-compile-chunk (2 before, 2 after)

Then, at the very end, when we're done compiling the chunk, we wrap things up:

^code finish-compile (1 before, 1 after)

That calls this:

^code end-compiler

Right now, our VM only knows expressions. When you run it, it parses, compiles,
and executes a single expression, then prints the result. To print that value,
we are temporarily using the `OP_RETURN` instruction. So we have the compiler
emit one of those at the end of the chunk:

^code emit-return

One last bonus function while we're at it:

^code emit-bytes

Over time, we'll have enough cases where we need to write an opcode followed by
a one-byte operand that it's worth making a helper function for it.

## Parsing Prefix Expressions

We've assembled our parsing and code generation functions. The only remaining
step in `compile()` that we need to implement is this function:

^code expression

We aren't ready to support every kind of expression in Lox yet. Heck, we don't
even have Booleans. For this chapter, we are only going to worry about:

* Number literals like `123`.
* Parentheses for grouping like `(123)`.
* Unary negation like `-123`.
* The Four Horsemen of binary arithmetic: `+`, `-`, `*`, `/`.

At this point, I'd usually show you the body of `expression()` and then we'd
work our way down from there, a function at a time. But a Pratt parser is built
around a table of function pointers and data. I don't want to dump the whole
table on you. We'll work our way to that by first showing the functions it
contains, then we'll stitch them all together.

So, bear with me. We're going to go through some more functions before we see
how they're actually called. Eventually, it will all come together. Feel free to
skip ahead and bounce around the chapter if it helps.

### Parsers for tokens

Imagine if every expression in Lox was only a single token. Each token type
represents a different kind of expression. For each expression, we need some
logic to output the appropriate bytecode. We can bundle the logic for each
expression type in a function. Then we build an array of function pointers. The
indexes in the array correspond to the `TokenType` enum values, and the function
at that index is the code to compile an expression of that type.

To compile a number literal, we store a pointer to this function in the
`TOKEN_NUMBER` offset in the array:

^code number

We assume the token for the number literal has already been advanced past, so
it's now stored in `previous`. We take the lexeme and use the C standard library
to convert it to a double value. Then we generate the code to load that value
using this function:

^code emit-constant

First we add the value to the constant table, then we emit an `OP_CONST`
instruction to push that constant value onto the stack. To add the constant, we
rely on:

^code make-constant

It uses `addConstant()` which we defined back in an [earlier chapter][bytecode].
That adds the given value to the end of the chunk's constant table and returns
its index.

[bytecode]: chunks-of-bytecode.html

There's also a little checking here to make sure we don't have too many
constants. Since the `OP_CONST` instruction uses a single byte for the index
operand, we can only store and load up to <span name="256">256</span> constants
in a chunk.

<aside name="256">

Yes, that limit is pretty low. If this were a full-sized language
implementation, we'd want to add another instruction like `OP_CONST_16` that
stores the index as a two-byte operand so we can handle more constants when
needed.

The code to support that isn't particularly illuminating, so I omitted it from
clox, but you'll want to scale to larger programs in your own VMs.

</aside>

That's basically all it takes. Provided there is some suitable code that
consumes a `TOKEN_NUMBER` token and then calls this, we can now compile number
literals to bytecode.

### Parentheses for grouping

The list of parsing function pointers works pretty well. It's easy to add new
syntax to the grammar by just plugging in a new function at the right index in
the array. It's also easy to see which tokens correspond to expressions by
seeing which array slots are empty.

Alas, not all expressions are a single token. However, many expressions *start*
with a particular token. We call these *prefix* expressions. For example, if we
trying to parse an expression, and the current token is `(`, we know we must be
looking at a parenthesized expression.

It turns out our function pointer array handles those too. The parsing function
for the expression can consume any additional tokens that it wants to, just like
in a regular recursive descent parser. Here's how parentheses work:

^code grouping

Again, it assumes the initial `(` has already been consumed. It <span
name="recursive">recursively</span> calls back into `expression()` to compile
the expression between the parentheses, then it parses the closing `)` at the
end.

<aside name="recursive">

A Pratt parser isn't a recursive *descent* parser, but it's still recursive.
That's to be expected since the grammar itself is recursive.

</aside>

As far as the back end is concerned, there's literally nothing to a grouping
expression. It's sole function is syntactic -- it lets you insert a lower
precedence expression where a higher precedence is expected. It has no runtime
semantics on its own and therefore it doesn't need to emit any bytecode. The
inner call to `expression()` takes care of generating bytecode for the
expression inside the parentheses.

### Unary negation

Unary minus is also a prefix expression, so it works with our model too:

^code unary

The leading `-` token has been consumed and is sitting in `parser.previous`. We
grab the token type from that to see which unary operator we're dealing with.
It's unnecessarily complex right now, but this will make more sense when we add
the `!` operator later.

Like in grouping, we recursively call `expression()` to compile the operand.
Then, after that, we emit the bytecode to perform the negation. It might seem a
little weird to emit that instruction after the operand since the `-` appears on
the left, but think about in terms of order of execution.

1. We need to evaluate the operand first and have its value on the stack.

2. Then we pop that value, negate it, and push the result.

So the `OP_NEGATE` instruction should be emitted last. This is a part of the
compiler's job -- parsing the program in the order it appears in the source code
and rearranging it to the order that execution happens.

There is one problem with this code, though. By calling `expression()`, we allow
any expression for the operand, regardless of precedence. Once we add binary
operators and other syntax, that will do the wrong thing. Consider:

```lox
-a.b + c;
```

Here, the operand to `-` should be just the `a.b` expression, not the entire
`a.b + c`. But if `unary()` calls `expression()`, it will happily chew through
all of that code including the `+`.

The operand to `-` is an expression, but it needs to only allow expressions at a
certain precedence level or higher. In jlox's recursive descent parser, each
method for parsing a specific expression also parses any expressions of higher
precedence too. That let us directly call that parsing method for the expression
type of the lowest allowed precedence and we'd get the others too.

The parsing functions here are different. Each only parses exactly one type of
expression. They don't cascade to include higher precedence expression types
too. So we need some kind of different solution. For now, let's just declare
that we have some function that parses any expression of a given precedence
level or higher:

^code parse-precedence

We will circle back and see *how* that function does what it does. For now, it's
magic. In order to take the "precedence" as a parameter, we define it
numerically:

^code precedence (1 before, 2 after)

These are all of Lox's precedence levels in order from lowest to highest. Since
C implicitly gives successively larger numbers for enums, this means that
`PREC_CALL` is numerically higher than `PREC_UNARY`. With this function in hand,
it's trivial to implement `expression()`:

^code expression-body (1 before, 1 after)

It just starts at the lowest precedence level, which subsumes all of the higher
precedence expressions too.

Now, to compile the operand for a unary expression, we call this new function
and limit it to the appropriate level:

^code unary-operand (1 before, 2 after)

Since unary operators have pretty high precedence, that correctly limits the
operand to a call expression or higher. So when we call `unary()` to parse:

```lox
-a.b + c;
```

It consumes the `a.b` and then stops there. Then `unary()` returns and relies on
some other code to handle parsing the surrounding `+` expression that contains
the negation on its left.

## Parsing Infix Operators

The remaining expressions are the binary arithmetic operators. These are
different from the previous expressions because they are *infix*. With the other
expressions, we know what kind of expression we have from its very first token.
Any other operand or syntax must follow it.

With infix expressions, we don't even know we're in the middle of a binary
operator until *after* we've parsed the left-hand side and then stumbled onto
the operator token in the middle.

```lox
-1 + 3
```

Let's walk through compiling this.

1.  We call `expression()`. It sees the first token is `-`. The associated parse
    function is `unary()`, so it calls that.

3.  `unary()` calls `parsePrecedence()` to parse the operand. That compiles the
    `1` literal. It does not compile the `+`, because that's too low precedence
    for the call to `parsePrecedence()`.

4.  `unary()` returns back to `expression()`.

Now what? Actually, we are right where we need to be. Now that we have compiled
the prefix expression, the next token is `+`. That's what the token we need to
realize that we're actually in the middle of an infix expression and that the
prefix expression we already called is actually an operand to that.

So we turn that array of function pointers into a *table*. One column associates
*prefix* parser functions with token types. This new column associates *infix*
parser functions with token types.

In the slot for `TOKEN_PLUS`, we point to this function:

^code binary

When a prefix parser function is called, the leading token has already been
consumed. An infix parser function is even more *in media res* -- the left-hand
operand has already been compiled and the infix operator consumed.

The fact that the left operand gets compiled first works out fine. It means at
runtime, that code gets executed first. When it runs, the value it produces will
end up on the stack. That's right where the infix operator is going to need it.
So even though the preceding expression is "contained" in this infix operation,
we don't need to even know that until after it's compiled.

Then we come here to `binary()` to handle the rest of the arithmetic operator.
We compile the right operand, much like how `unary()` compiled its own trailing
operand. Finally, we emit the bytecode instruction that performs the binary
operation.

When run, we'll execute the left and right operand expressions, in that order,
leaving their values on the stack. Then we execute the instruction for the
operator. That pops the two values, combines them, and pushes the result.

The code that probably caught your eye here is that `getRule()` line. When we
parse the right-hand operand, we again need to worry about precedence. Take an
expression like:

```lox
2 * 3 + 4
```

When we parse the right operand of the `*` expression, we need to just capture
`3`, and not `3 + 4`, because `+` is lower precedence than `*`. We could make
separate `binary()` functions for each binary operator and call
`parsePrecedence()`, passing in the next level of precedence above that
operator.

But that's kind of tedious. Instead, we look up the current operator's own
precedence using this `getRule()` thing we'll get to soon. Using that, we call
`parsePrecedence()` with <span name="higher">one</span> level higher than this
operator's. For example, `*` has precedence `PREC_FACTOR`. The next level up is
`PREC_UNARY`, which is indeed higher than `+`, so it ensures we don't consume
the `+ 4` part.

<aside name="higher">

Adding one to the precedence does assume that the binary operator is
left-associative. Given a series of the *same* operator, like:

```lox
1 + 2 + 3 + 4
```

We want it to be parsed like:

```lox
((1 + 2) + 3) + 4
```

Thus, when parsing the right-hand operand to the first `+`, we want to consume
the `2`, but not the rest. Passing in `PREC_TERM + 1` does that. But if our
operator was *right*-associative, like `=` is, then this would do the wrong
thing. Given:

```lox
a = b = c = d
```

We want it to be parsed like:

```lox
a = (b = (c = d))
```

When parsing the right operand of the first `=`, we *should* consume the entire
expression, including the other `=`. To enable that, we would call
`parsePrecedence()` with the *same* precedence as the current operator.

</aside>

This way, we can use a single `binary()` function for all binary arithmetic
operators even though they have different precedences.

## A Pratt Parser

We now have all of the pieces and parts of the compiler laid out. We have a
function for each grammar production: `number()`, `grouping()`, `unary()`, and
`binary()`. It's time to stitch it all together and see what `expression()`,
`parsePrecedence()`, and `getRule()` look like.

We know we need some sort of table that, given a token type, lets us look up:

*   The function to compile a prefix expression starting with a token of that
    type.

*   The function to compile an infix expression that has an operand followed by
    an expression of this type.

*   The precedence of an <span name="prefix">infix</span> expression that uses
    this token as an operator.

<aside name="prefix">

We don't need to store the precedence of the *prefix* expression starting
with a given token because all prefix operators in Lox have the same
precedence.

</aside>

We wrap these three properties in a little struct:

^code parse-rule (1 before, 2 after)

The table that drives our whole parser is an array of ParseRules. The index in
the array corresponds to a TokenType enum value, and the ParseRule at that index
identifies the prefix parse function, infix parse function, and infix precedence
for a token of that type.

That ParseFn type is a simple <span name="typedef">typedef</span> for a function
type that takes no arguments and returns nothing:

<aside name="typedef">

C's syntax for function pointer types is so bad I never use it inline. I always
define a typedef. I understand the theory behind the syntax, the whole
"declaration reflects use" thing, but I think it was a failed syntactic
experiment.

</aside>

^code parse-fn-type (1 before, 2 after)

The actual table is this big static array:

^code rules

<aside name="big">

See what I mean about not wanting to revisit it each time we needed a new
column? It's a beast.

</aside>

There are a lot of `NULL` and `PREC_NONE` values in here. Some of those are
because there is no expression associated with many tokens. You can't start an
expression with, say, `else`, and `}` would make for a pretty confusing infix
operator.

But, also, we haven't filled in the entire grammar yet. In later chapters, as we
add new expression types, some of these slots will get real functions in them.
In fact, one of the things I like about this approach to parsing is that it
makes it very easy to see which tokens are in use by the grammar and which are
available.

Now that we have the table, we are finally ready to write the code that uses it.
This is where our Pratt parser comes to life. The simplest function to define is
`getRule()`:

^code get-rule

It simply returns the rule at the given index. It's called by `binary()` to look
up the precedence of the current operator. This function exists to handle a
declaration cycle in the C code. `binary()` is defined *before* the rules table
so that the table can store a pointer to it. That means the body of `binary()`
cannot access the table directly.

Instead, we wrap the lookup in a function. That lets us forward declare
`getRule()` before the definition of `binary()`, and <span
name="forward">then</span> *define* `getRule()` after the table.

<aside name="forward">

This is what happens when you write your VM in a language that was designed to
be compiled on a PDP-11 that didn't have enough memory to read an entire source
file.

</aside>

We'll need a couple of other forward declarations to handle the fact that our
grammar is recursive, so let's get them all out of the way:

^code forward-declarations (2 before, 1 after)

If you're following and implementing clox yourself, pay close attention to the
little annotations that tell you where to put these code snippets. If you get it
wrong, the C compiler will be happy to tell you.

### Parsing with precedence

Now we're getting to the fun stuff. The maestro that orchestrates all of the
parsing functions we've defined is `parsePrecedence()`. Let's start with parsing
prefix expressions:

^code precedence-body (1 before, 1 after)

It reads the next token and looks up the corresponding ParseRule. If there is no
prefix parser for the token, then the code contains a syntax error. We report
that and return to the caller.

Otherwise, we call that prefix parse function and let it do its thing. That
prefix parser compiles the rest of the expression, consumes any other tokens it
needs, and returns back here. That's really all there is to it, at least for
prefix expressions.

Infix expressions are where it gets a little subtle, because they are where
precedence comes into play. The implementation is deceptively simple:

^code infix (1 before, 1 after)

That's the whole thing. Really. Look at the rest of the chapter.

Here's how the complete `parsePrecedence()` function works. When it parses an
expression, the very first token always belongs to some prefix expression. It
may be deeply nested inside one or more infix expressions, but the leftmost
token ultimately belongs to some prefix expression.

Let's walk through a simple example:

```lox
-1 + 2
```

**todo: illustrate ast**

Here, the outermost expression is the addition. The `-1` is the left operand to
it. We call `expression()` to compile this. That forwards to
`parsePrecedence(PREC_ASSIGN)` which reads the first token, `-`. It finds the
`unary()` prefix parser associated with `TOKEN_MINUS` in the ParseRule table and
calls that.

**todo: walk through steps in sidebar?**

That recursively calls `parsePrecedence(PREC_CALL)`. Again, it reads the next
token, this time `1`. It calls `number()` the prefix parser for that which
compiles the number literal but doesn't consume any other tokens.

Then the inner call to `parsePrecedence(PREC_CALL)` reaches the while loop. The
current token is `+`. It looks up the precedence for that, which is `PREC_TERM`.
That's lower than `PREC_CALL`, so we don't enter the loop, and this call to
`parsePrecedence(PREC_CALL)` returns. See how that precedence comparison ensures
that don't incorrectly compile the `+` operator as an operand to `-`? That's the
trick right there.

Now we're back in the outer call to `parsePrecedence(PREC_ASSIGN)`. Again, we
reach the while loop. We're still on the `+` token. This time, the precedence we
are comparing to is `PREC_ASSIGN` which is lower than `PREC_TERM`. We enter the
loop and consume the `+` token. We look up the infix rule, find `binary()`, and
call it.

The left operand has already been compiled. We call `parsePrecendence()` again
for the right operand. That compiles the `2` number literal as a prefix
expression. The next token is `TOKEN_EOF`, which has the special super-low
precedence `PREC_NONE`. That keeps us from getting into the infix while loop, so
we return back to the outer call to `parsePrecedence()`.

That jumps back to the top of the loop. The current token is still `TOKEN_EOF`,
so we exit that loop too and finish, returning back to `expression()`. And we're
done.

The code is simple, but it's recursive and uses function pointers, so it can
take a while to wrap your head around it. It's worth stepping through it in a
debugger while it compiles a few different expressions. The basic idea is:

1.  Parse the prefix expression starting at the current token.

2.  Then, as long as the next token is an infix expression whose precedence is
    high enough compile it too.

Once it clicks for you, you'll be able to implement this pattern whenever you
need an expression parser. We'll need to tweak the code a little to handle
assignment, but, otherwise, the code we have here covers all of our expression
compiling needs for the rest of the book. We'll plug additional parsing
functions into the table as we add new kinds of expressions, but
`parsePrecedence()` is complete.

## Dumping Chunks

The core of the expression compiler is essentially complete, but while we're in
here, we should add another diagnostic tool for ourselves. To help debug the
generated bytecode, we'll add support for dumping the chunk once the compiler
finishes. We did that earlier when we hand-authored a chunk. Now we'll hook up
some code so that we can turn that on whenever we want.

Since this isn't for end users, we hide it behind a flag:

^code define-debug-print-code (2 before, 1 after)

When that flag is defined, we use our existing debug code to print out the
chunk's bytecode:

^code dump-chunk (1 before, 1 after)

We only do this if the code compiled without any errors. When there is a compile
error, the compiler keeps on going but it's in kind of a weird state and might
produce broken code. That's safe, because it won't get executed, but we'll just
confuse ourselves if we try to read it.

To access `disassembleChunk()`, we need to include the module:

^code include-debug (1 before, 2 after)

We made it! This was the last major section to install in our VM's compilation
and execution pipeline. It doesn't *look* like much, but inside it is scanning,
parsing, generating bytecode, and executing it.

Fire up the VM and type in an expression. If we did everything right, it should
calculate and print the result. We now have a very over-engineered integer
arithmetic calculator. We have a lot of language features to add in the coming chapters, but the foundation is in place.

<div class="challenges">

## Challenges

1.  The ParseRule row for `TOKEN_MINUS` has both prefix and infix function
    pointers. That's because `-` is both a prefix operator (unary negation) and
    infix (subtraction).

    In the full Lox language, what other tokens can be used in both prefix and
    infix positions? What about in C or another language of your choice?

2.  You might be wondering about more complex "mixfix" expressions that have
    more than two operands separated by tokens. C's "conditional" or "ternary"
    operator, `? :` is a widely-known one.

    Add support for that operator to the compiler here. You don't have to
    generate any bytecode, just show how you would hook it up to the parser and
    handle the operands.

3.  Likewise, add support for prefix and postfix `++` operators. Again, don't
    worry about bytecode (we don't even have variables to assign yet).

</div>
