Since our interpreter is so small, the change is pretty straightforward. First,
we declare a local variable for the `ip` of the current CallFrame:

```c
static InterpretResult run() {
  CallFrame* frame = &vm.frames[vm.frameCount - 1];
  register uint8_t* ip = frame->ip; // <-- Add.
```

We replace the macros to read from that:

```c
#define READ_BYTE() (*ip++)
#define READ_SHORT() \
    (ip += 2, (uint16_t)((ip[-2] << 8) | ip[-1]))
```

Then the jump instructions write to it:

```c
      case OP_JUMP: {
        uint16_t offset = READ_SHORT();
        ip += offset;
        break;
      }

      case OP_JUMP_IF_FALSE: {
        uint16_t offset = READ_SHORT();
        if (isFalsey(peek(0))) ip += offset;
        break;
      }

      case OP_LOOP: {
        uint16_t offset = READ_SHORT();
        ip -= offset;
        break;
      }
```

Cache invalidation is the harder part. Before a call, we store the `ip` back
into the frame in case the call pushes a new frame. Then we load the `ip` of
the new frame once the call has pushed it:

```c
      case OP_CALL: {
        int argCount = READ_BYTE();
        frame->ip = ip; // <-- Add.
        if (!callValue(peek(argCount), argCount)) {
          return INTERPRET_RUNTIME_ERROR;
        }
        frame = &vm.frames[vm.frameCount - 1];
        ip = frame->ip; // <-- Add.
        break;
      }
```

Likewise, on a return, we need to reload the `ip` of the CallFrame we're
returning to:

```c
        frame = &vm.frames[vm.frameCount - 1];
        ip = frame->ip; // <-- Add.
        break;
```

On my machine, this reduce the execution time of a simple Fibonacci benchmark by
about 8.5%. That doesn't sound like a huge amount, but many language
implementers would be thrilled to find an optimization that juicy. If you run
the VM in a profiler, you'll see a good chunk of the execution time is spent
looking up `fib` in the global variable hash table, so speeding up calls is only
going to buy us so much.

I definitely think this is worth it.
